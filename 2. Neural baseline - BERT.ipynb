{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import razdel\n",
    "import fasttext\n",
    "import os\n",
    "import json\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "data = pd.read_pickle('data/train.annotated.pkl')\n",
    "data['tokens'] = data.annot.map(lambda row: ' '.join([tok.text for tok in row['tokens']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "      <th>annot</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17024</td>\n",
       "      <td>[USER], согласно предписаниям Роспотребнадзора...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{'text': 'согласно предписаниям Роспотребнадзо...</td>\n",
       "      <td>согласно предписаниям Роспотребнадзора , все т...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  masks_stance  \\\n",
       "0    17024  [USER], согласно предписаниям Роспотребнадзора...            -1   \n",
       "\n",
       "   masks_argument  quarantine_stance  quarantine_argument  vaccines_stance  \\\n",
       "0              -1                  1                    1               -1   \n",
       "\n",
       "   vaccines_argument                                              annot  \\\n",
       "0                 -1  {'text': 'согласно предписаниям Роспотребнадзо...   \n",
       "\n",
       "                                              tokens  \n",
       "0  согласно предписаниям Роспотребнадзора , все т...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_mapper = {\n",
    "    'ё': 'е',\n",
    "    '“': '«',\n",
    "    '”': '»'\n",
    "}\n",
    "\n",
    "data.tokens = data.tokens.replace(symbol_mapper, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtokentome as yttm\n",
    "import re\n",
    "\n",
    "MODELNAME = 'bpe.model'\n",
    "\n",
    "\n",
    "def train_bpe(texts, vocab_size=10000):\n",
    "    texts_filename = 'texts.txt'\n",
    "    with open(texts_filename, 'w') as f:\n",
    "        for text in texts:\n",
    "            f.write(text + '\\n')\n",
    "    \n",
    "    yttm.BPE.train(data=texts_filename, vocab_size=vocab_size, model=MODELNAME)\n",
    "            \n",
    "train_bpe(data.tokens.map(lambda row: row.lower()))\n",
    "bpe = yttm.BPE(model=MODELNAME)\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    return ' '.join(bpe.encode(text, output_type=yttm.OutputType.SUBWORD))\n",
    "\n",
    "data['bpe'] = data.tokens.map(lambda row: bpe_tokenize(row.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "      <th>annot</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17024</td>\n",
       "      <td>[USER], согласно предписаниям Роспотребнадзора...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{'text': 'согласно предписаниям Роспотребнадзо...</td>\n",
       "      <td>согласно предписаниям Роспотребнадзора , все т...</td>\n",
       "      <td>▁согласно ▁предписа ниям ▁роспотребнадзора ▁, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  masks_stance  \\\n",
       "0    17024  [USER], согласно предписаниям Роспотребнадзора...            -1   \n",
       "\n",
       "   masks_argument  quarantine_stance  quarantine_argument  vaccines_stance  \\\n",
       "0              -1                  1                    1               -1   \n",
       "\n",
       "   vaccines_argument                                              annot  \\\n",
       "0                 -1  {'text': 'согласно предписаниям Роспотребнадзо...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  согласно предписаниям Роспотребнадзора , все т...   \n",
       "\n",
       "                                                 bpe  \n",
       "0  ▁согласно ▁предписа ниям ▁роспотребнадзора ▁, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "i = 0\n",
    "for train, test in kf.split(data):\n",
    "    pathname = f'data/fold_{i}'\n",
    "    if os.path.isdir(pathname):\n",
    "        import shutil\n",
    "        shutil.rmtree(pathname)\n",
    "    \n",
    "    os.mkdir(pathname)\n",
    "    \n",
    "    data.iloc[train].reset_index(drop=True).to_pickle(os.path.join(pathname, 'train.pkl'))\n",
    "    data.iloc[test].reset_index(drop=True).to_pickle(os.path.join(pathname, 'test.pkl'))\n",
    "\n",
    "    bpe_data = pd.DataFrame({\n",
    "        'text': data.bpe,\n",
    "        'masks_stance': data.masks_stance,\n",
    "        'masks_argument': data.masks_argument,\n",
    "        'quarantine_stance': data.quarantine_stance,\n",
    "        'quarantine_argument': data.quarantine_argument,\n",
    "        'vaccines_stance': data.vaccines_stance,\n",
    "        'vaccines_argument': data.vaccines_argument\n",
    "    })\n",
    "    \n",
    "    with open(os.path.join(pathname, 'train_bpe.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(json.dumps(i) for i in bpe_data.iloc[train].to_dict('records')) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(pathname, 'test_bpe.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(json.dumps(i) for i in bpe_data.iloc[test].to_dict('records')) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(pathname, 'train_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(json.dumps(i) for i in data[['tokens',\n",
    "                                                        'masks_stance', 'masks_argument',\n",
    "                                                        'quarantine_stance', 'quarantine_argument',\n",
    "                                                        'vaccines_stance', 'vaccines_argument'\n",
    "                                                       ]].iloc[train].to_dict('records')) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(pathname, 'test_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(json.dumps(i) for i in data[['tokens',\n",
    "                                                        'masks_stance', 'masks_argument',\n",
    "                                                        'quarantine_stance', 'quarantine_argument',\n",
    "                                                        'vaccines_stance', 'vaccines_argument'\n",
    "                                                       ]].iloc[test].to_dict('records')) + '\\n')\n",
    "        \n",
    "    i += 1\n",
    "    print(data.iloc[train].shape, data.iloc[test].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 54M\n",
      "-rw-r--r-- 1 root root 809K Jan 30 12:07 test_quarantine_tokens.json\n",
      "drwxr-xr-x 2 root root 4.0K Jan 30 12:07 .\n",
      "-rw-r--r-- 1 root root 3.2M Jan 30 12:07 train_quarantine_tokens.json\n",
      "-rw-r--r-- 1 root root 809K Jan 28 15:35 test_vaccines_tokens.json\n",
      "-rw-r--r-- 1 root root 3.2M Jan 28 15:35 train_vaccines_tokens.json\n",
      "-rw-r--r-- 1 root root 808K Jan 28 11:54 test_mask_tokens.json\n",
      "-rw-r--r-- 1 root root 3.2M Jan 28 11:54 train_mask_tokens.json\n",
      "drwxr-xr-x 7 root root 4.0K Jan 28 11:52 ..\n",
      "-rw-r--r-- 1 root root 954K Jan 28 11:52 test_tokens.json\n",
      "-rw-r--r-- 1 root root 3.8M Jan 28 11:52 train_tokens.json\n",
      "-rw-r--r-- 1 root root 1.1M Jan 28 11:52 test_bpe.json\n",
      "-rw-r--r-- 1 root root 4.4M Jan 28 11:52 train_bpe.json\n",
      "-rw-r--r-- 1 root root 6.3M Jan 28 11:52 test.pkl\n",
      "-rw-r--r-- 1 root root  26M Jan 28 11:52 train.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls -laht data/fold_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main performance metric in each of the two tasks is the macro F1-score (macro F1rel-score), which is averaged first over three relevance classes (the class “irrelevant” is excluded), and then over topics. More precisely, the following procedure is used:\n",
    "\n",
    "- for each of the three claims, F1-score is calculated for each class (label) separately;\n",
    "- F1-scores are averaged over three out of four classes (the “irrelevant” class is excluded) – macro F1rel-score is obtained for a given claim; # aka fine_grained_f1\n",
    "- macro F1rel-scores for all three claims are averaged – we get macro F1rel-score relative to the task (stance detection or premise classification). # average_f1\n",
    "\n",
    "As a result, two main macro F1rel-scores will be calculated – one for each task. Participants’ systems will be ranked by these metrics (two separate lists). The F1rel-score for claims and F1-score for individual classes (labels) will be considered auxiliary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def fine_grained_f1(true, pred):\n",
    "    labels = [0, 1, 2]\n",
    "    return f1_score(true, pred, average='macro', labels=labels)\n",
    "\n",
    "def average_f1(f1s):\n",
    "    return np.average(f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RuBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'configs'\n",
    "\n",
    "if not os.path.isdir(CONFIG_PATH):\n",
    "    os.mkdir(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a Masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train/test files for mask-related classification: 100%|██████████| 5/5 [00:00<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold in tqdm(range(5), desc=\"Writing train/test files for mask-related classification\"):\n",
    "    pathname = f'data/fold_{fold}'\n",
    "    \n",
    "    with open(f'data/fold_{fold}/train_tokens.json', 'r') as file:\n",
    "        train = pd.read_json(file.read(), lines=True)\n",
    "    \n",
    "    train['text'] = train.tokens.map(lambda row: row.lower())\n",
    "    train['label1'] = train.masks_stance.map(str)\n",
    "    train['label2'] = train.masks_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'train_mask_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in train[['text', 'label1', 'label2']].to_dict('records')) + '\\n')\n",
    "       \n",
    "    with open(f'data/fold_{fold}/test_tokens.json', 'r') as file:\n",
    "        test = pd.read_json(file.read(), lines=True)\n",
    "        \n",
    "    test['text'] = test.tokens.map(lambda row: row.lower())\n",
    "    test['label1'] = test.masks_stance.map(str)\n",
    "    test['label2'] = test.masks_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'test_mask_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in test[['text', 'label1', 'label2']].to_dict('records')) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/rubert_masks_0.jsonnet\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/rubert_masks_0.jsonnet\n",
    "\n",
    "local embedding_dim = 768;\n",
    "local foldnum = 0;\n",
    "local max_length = 512;\n",
    "local lr = std.parseJson(std.extVar('lr'));\n",
    "local dropout = std.parseJson(std.extVar('dropout'));\n",
    "local batch_size = std.parseJson(std.extVar('batch_size'));\n",
    "local dataset_reader_type = \"models_scripts.dataset_readers.two_outputs_rdr.TwoOutputsTextClassificationJsonReader\";\n",
    "local model_type = \"models_scripts.models.two_outputs_clf.TwoOutputsTextClassifier\";\n",
    "local model_name = \"DeepPavlov/rubert-base-cased\";\n",
    "\n",
    "{\n",
    "  vocabulary: {\n",
    "    non_padded_namespaces: [\"tokens\", \"labels1\", \"labels2\"]\n",
    "  },\n",
    "  dataset_reader: {\n",
    "      type: dataset_reader_type,\n",
    "      num_labels1: 4,\n",
    "      num_labels2: 4,\n",
    "      tokenizer: {\n",
    "        type: \"pretrained_transformer\",\n",
    "        model_name: model_name,\n",
    "        max_length: max_length,\n",
    "      },\n",
    "      token_indexers: {\n",
    "          tokens: {\n",
    "            type: \"pretrained_transformer\",\n",
    "            model_name: model_name,\n",
    "            max_length: max_length,\n",
    "            namespace: 'tokens',\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  train_data_path: 'data/fold_' + foldnum + '/train_mask_tokens.json',\n",
    "  validation_data_path: 'data/fold_' + foldnum + '/test_mask_tokens.json',\n",
    "  model: {\n",
    "    type: model_type,\n",
    "    dropout: dropout,\n",
    "    num_labels1: 4,\n",
    "    num_labels2: 4,\n",
    "    label1_weights: [0.2, 0.3, 1.0, 1.0],\n",
    "    label2_weights: [0.1, 0.1, 1.0, 1.0],\n",
    "    text_field_embedder: {\n",
    "        token_embedders: {\n",
    "            tokens: {\n",
    "              type: \"pretrained_transformer\",\n",
    "              model_name: model_name,\n",
    "              max_length: max_length,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    seq2vec_encoder: {\n",
    "        type: \"bert_pooler\",\n",
    "           pretrained_model: model_name,\n",
    "           requires_grad: true,\n",
    "           dropout: 0.2,\n",
    "    }\n",
    "  },\n",
    "  data_loader: {\n",
    "    batch_sampler: {\n",
    "      type: 'bucket',\n",
    "      batch_size: batch_size,\n",
    "    },\n",
    "  },\n",
    "  trainer: {\n",
    "    optimizer: {\n",
    "      type: \"huggingface_adamw\",\n",
    "      lr: lr,\n",
    "      weight_decay: 0.1,\n",
    "    },\n",
    "    learning_rate_scheduler: {\n",
    "      type: \"slanted_triangular\",\n",
    "      cut_frac: 0.06\n",
    "    },\n",
    "    validation_metric: '+all_mean',\n",
    "    num_serialized_models_to_keep: 1,\n",
    "    num_epochs: 30,\n",
    "    patience: 3,\n",
    "    cuda_device: 1,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/convbert_masks_0.jsonnet\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/convbert_masks_0.jsonnet\n",
    "\n",
    "local embedding_dim = 768;\n",
    "local foldnum = 0;\n",
    "local max_length = 512;\n",
    "local lr = std.parseJson(std.extVar('lr'));\n",
    "local dropout = std.parseJson(std.extVar('dropout'));\n",
    "local batch_size = std.parseJson(std.extVar('batch_size'));\n",
    "local dataset_reader_type = \"models_scripts.dataset_readers.two_outputs_rdr.TwoOutputsTextClassificationJsonReader\";\n",
    "local model_type = \"models_scripts.models.two_outputs_clf.TwoOutputsTextClassifier\";\n",
    "local model_name = \"DeepPavlov/rubert-base-cased-conversational\";\n",
    "\n",
    "{\n",
    "  vocabulary: {\n",
    "    non_padded_namespaces: [\"tokens\", \"labels1\", \"labels2\"]\n",
    "  },\n",
    "  dataset_reader: {\n",
    "      type: dataset_reader_type,\n",
    "      num_labels1: 4,\n",
    "      num_labels2: 4,\n",
    "      tokenizer: {\n",
    "        type: \"pretrained_transformer\",\n",
    "        model_name: model_name,\n",
    "        max_length: max_length,\n",
    "      },\n",
    "      token_indexers: {\n",
    "          tokens: {\n",
    "            type: \"pretrained_transformer\",\n",
    "            model_name: model_name,\n",
    "            max_length: max_length,\n",
    "            namespace: 'tokens',\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  train_data_path: 'data/fold_' + foldnum + '/train_mask_tokens.json',\n",
    "  validation_data_path: 'data/fold_' + foldnum + '/test_mask_tokens.json',\n",
    "  model: {\n",
    "    type: model_type,\n",
    "    dropout: dropout,\n",
    "    num_labels1: 4,\n",
    "    num_labels2: 4,\n",
    "    label1_weights: [0.2, 0.3, 1.0, 1.0],\n",
    "    label2_weights: [0.1, 0.1, 1.0, 1.0],\n",
    "    text_field_embedder: {\n",
    "        token_embedders: {\n",
    "            tokens: {\n",
    "              type: \"pretrained_transformer\",\n",
    "              model_name: model_name,\n",
    "              max_length: max_length,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    seq2vec_encoder: {\n",
    "        type: \"bert_pooler\",\n",
    "           pretrained_model: model_name,\n",
    "           requires_grad: true,\n",
    "           dropout: 0.2,\n",
    "    }\n",
    "  },\n",
    "  data_loader: {\n",
    "    batch_sampler: {\n",
    "      type: 'bucket',\n",
    "      batch_size: batch_size,\n",
    "    },\n",
    "  },\n",
    "  trainer: {\n",
    "    optimizer: {\n",
    "      type: \"huggingface_adamw\",\n",
    "      lr: lr,\n",
    "      weight_decay: 0.1,\n",
    "    },\n",
    "    learning_rate_scheduler: {\n",
    "      type: \"slanted_triangular\",\n",
    "      cut_frac: 0.06\n",
    "    },\n",
    "    validation_metric: '+all_mean',\n",
    "    num_serialized_models_to_keep: 1,\n",
    "    num_epochs: 30,\n",
    "    patience: 3,\n",
    "    cuda_device: 1,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/base_model_params.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/base_model_params.json\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"type\": \"int\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"batch_size\",\n",
    "      \"low\": 2,\n",
    "      \"high\": 8\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"float\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"dropout\",\n",
    "      \"low\": 0.0,\n",
    "      \"high\": 0.5\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"float\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"lr\",\n",
    "      \"low\": 2e-6,\n",
    "      \"high\": 2e-4,\n",
    "      \"log\": true\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tune_convbert.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile tune_convbert.sh\n",
    "\n",
    "export METHOD=convbert_masks\n",
    "# rm -r $METHOD\n",
    "# mkdir $METHOD\n",
    "export FOLD=0\n",
    "export STUDY_NAME=convbert_test30\n",
    "# optuna delete-study --study-name $STUDY_NAME\n",
    "allennlp tune configs/${METHOD}_${FOLD}.jsonnet configs/base_model_params.json --serialization-dir $METHOD/fold_${FOLD} \\\n",
    "    --study-name $STUDY_NAME \\\n",
    "    --skip-if-exists \\\n",
    "    --metrics best_validation_all_mean \\\n",
    "    --direction maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 14:46:10,137 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2022-01-28 14:46:10,773 - INFO - allennlp.common.plugins - Plugin allennlp_optuna available\n",
      "batch_size=4 dropout=0.4424265997222608 lr=2.533292198731432e-05\n"
     ]
    }
   ],
   "source": [
    "! allennlp best-params --study-name convbert_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def find_top_trials(path):\n",
    "    mean_all = dict()\n",
    "    for directory in glob.glob(path):\n",
    "        try:\n",
    "            metrics = json.load(open(os.path.join(directory, 'metrics.json'), 'r'))\n",
    "            mean_all[directory] = (metrics.get('best_validation_all_mean'), \n",
    "                                   metrics.get('best_validation_f1_1_mean'),\n",
    "                                   metrics.get('best_validation_f1_2_mean'))\n",
    "        except:\n",
    "            pass\n",
    "    return {key: value for key, value in sorted(mean_all.items(), key=lambda x: x[1][0], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_top_trials('convbert_masks/fold_0/trial_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting convbert_masks.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile convbert_masks.sh\n",
    "\n",
    "export METHOD=convbert_masks\n",
    "rm -r $METHOD\n",
    "mkdir $METHOD\n",
    "export batch_size=4\n",
    "export dropout=0.44\n",
    "export lr=2.5e-05\n",
    "\n",
    "python utils/make_k_copies.py --filename configs/${METHOD}_0.jsonnet --k 5\n",
    "\n",
    "export FOLD=0\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=1\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=2\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=3\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=4\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_mask_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_mask_tokens.json \\\n",
    "                 --include-package models_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b Vaccines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train/test files for vaccines-related classification: 100%|██████████| 5/5 [00:00<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold in tqdm(range(5), desc=\"Writing train/test files for vaccines-related classification\"):\n",
    "    pathname = f'data/fold_{fold}'\n",
    "    \n",
    "    with open(f'data/fold_{fold}/train_tokens.json', 'r') as file:\n",
    "        train = pd.read_json(file.read(), lines=True)\n",
    "    \n",
    "    train['text'] = train.tokens.map(lambda row: row.lower())\n",
    "    train['label1'] = train.vaccines_stance.map(str)\n",
    "    train['label2'] = train.vaccines_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'train_vaccines_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in train[['text', 'label1', 'label2']].to_dict('records')) + '\\n')\n",
    "       \n",
    "    with open(f'data/fold_{fold}/test_tokens.json', 'r') as file:\n",
    "        test = pd.read_json(file.read(), lines=True)\n",
    "        \n",
    "    test['text'] = test.tokens.map(lambda row: row.lower())\n",
    "    test['label1'] = test.vaccines_stance.map(str)\n",
    "    test['label2'] = test.vaccines_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'test_vaccines_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in test[['text', 'label1', 'label2']].to_dict('records')) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/convbert_vaccines_0.jsonnet\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/convbert_vaccines_0.jsonnet\n",
    "\n",
    "local embedding_dim = 768;\n",
    "local foldnum = 0;\n",
    "local max_length = 512;\n",
    "local lr = std.parseJson(std.extVar('lr'));\n",
    "local dropout = std.parseJson(std.extVar('dropout'));\n",
    "local batch_size = std.parseJson(std.extVar('batch_size'));\n",
    "local dataset_reader_type = \"models_scripts.dataset_readers.two_outputs_rdr.TwoOutputsTextClassificationJsonReader\";\n",
    "local model_type = \"models_scripts.models.two_outputs_clf.TwoOutputsTextClassifier\";\n",
    "local model_name = \"DeepPavlov/rubert-base-cased-conversational\";\n",
    "\n",
    "{\n",
    "  vocabulary: {\n",
    "    non_padded_namespaces: [\"tokens\", \"labels1\", \"labels2\"]\n",
    "  },\n",
    "  dataset_reader: {\n",
    "      type: dataset_reader_type,\n",
    "      num_labels1: 4,\n",
    "      num_labels2: 4,\n",
    "      tokenizer: {\n",
    "        type: \"pretrained_transformer\",\n",
    "        model_name: model_name,\n",
    "        max_length: max_length,\n",
    "      },\n",
    "      token_indexers: {\n",
    "          tokens: {\n",
    "            type: \"pretrained_transformer\",\n",
    "            model_name: model_name,\n",
    "            max_length: max_length,\n",
    "            namespace: 'tokens',\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  train_data_path: 'data/fold_' + foldnum + '/train_vaccines_tokens.json',\n",
    "  validation_data_path: 'data/fold_' + foldnum + '/test_vaccines_tokens.json',\n",
    "  model: {\n",
    "    type: model_type,\n",
    "    dropout: dropout,\n",
    "    num_labels1: 4,\n",
    "    num_labels2: 4,\n",
    "    label1_weights: [0.1, 0.4, 1.0, 1.0],\n",
    "    label2_weights: [0.1, 0.1, 0.6, 1.0],\n",
    "    text_field_embedder: {\n",
    "        token_embedders: {\n",
    "            tokens: {\n",
    "              type: \"pretrained_transformer\",\n",
    "              model_name: model_name,\n",
    "              max_length: max_length,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    seq2vec_encoder: {\n",
    "        type: \"bert_pooler\",\n",
    "           pretrained_model: model_name,\n",
    "           requires_grad: true,\n",
    "           dropout: 0.2,\n",
    "    }\n",
    "  },\n",
    "  data_loader: {\n",
    "    batch_sampler: {\n",
    "      type: 'bucket',\n",
    "      batch_size: batch_size,\n",
    "    },\n",
    "  },\n",
    "  trainer: {\n",
    "    optimizer: {\n",
    "      type: \"huggingface_adamw\",\n",
    "      lr: lr,\n",
    "      weight_decay: 0.1,\n",
    "    },\n",
    "    learning_rate_scheduler: {\n",
    "      type: \"slanted_triangular\",\n",
    "      cut_frac: 0.06\n",
    "    },\n",
    "    validation_metric: '+all_mean',\n",
    "    num_serialized_models_to_keep: 1,\n",
    "    num_epochs: 30,\n",
    "    patience: 3,\n",
    "    cuda_device: 1,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tune_convbert.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile tune_convbert.sh\n",
    "\n",
    "export METHOD=convbert_vaccines\n",
    "# rm -r $METHOD\n",
    "# mkdir $METHOD\n",
    "export FOLD=0\n",
    "export STUDY_NAME=convbert_test3\n",
    "# optuna delete-study --study-name $STUDY_NAME\n",
    "allennlp tune configs/${METHOD}_${FOLD}.jsonnet configs/base_model_params.json --serialization-dir $METHOD/fold_${FOLD} \\\n",
    "    --study-name $STUDY_NAME \\\n",
    "    --skip-if-exists \\\n",
    "    --metrics best_validation_all_mean \\\n",
    "    --direction maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 11:08:21,283 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2022-01-30 11:08:22,059 - INFO - allennlp.common.plugins - Plugin allennlp_optuna available\n",
      "batch_size=8 dropout=0.16807920937155635 lr=5.389378433438082e-06\n"
     ]
    }
   ],
   "source": [
    "! allennlp best-params --study-name convbert_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_trials('convbert_vaccines/fold_0/trial_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting convbert_vaccines.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile convbert_vaccines.sh\n",
    "\n",
    "export METHOD=convbert_vaccines\n",
    "rm -r $METHOD\n",
    "mkdir $METHOD\n",
    "export batch_size=8\n",
    "export dropout=0.2\n",
    "export lr=5e-06\n",
    "\n",
    "python utils/make_k_copies.py --filename configs/${METHOD}_0.jsonnet --k 5\n",
    "\n",
    "export FOLD=0\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=1\n",
    "rm -r $METHOD/fold_${FOLD}\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=2\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=3\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=4\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_vaccines_tokens.json \\\n",
    "                 --include-package models_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c Quarantine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train/test files for quarantine-related classification: 100%|██████████| 5/5 [00:01<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold in tqdm(range(5), desc=\"Writing train/test files for quarantine-related classification\"):\n",
    "    pathname = f'data/fold_{fold}'\n",
    "    \n",
    "    with open(f'data/fold_{fold}/train_tokens.json', 'r') as file:\n",
    "        train = pd.read_json(file.read(), lines=True)\n",
    "    \n",
    "    train['text'] = train.tokens.map(lambda row: row.lower())\n",
    "    train['label1'] = train.quarantine_stance.map(str)\n",
    "    train['label2'] = train.quarantine_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'train_quarantine_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in train[['text', 'label1', 'label2']].to_dict('records')) + '\\n')\n",
    "       \n",
    "    with open(f'data/fold_{fold}/test_tokens.json', 'r') as file:\n",
    "        test = pd.read_json(file.read(), lines=True)\n",
    "        \n",
    "    test['text'] = test.tokens.map(lambda row: row.lower())\n",
    "    test['label1'] = test.quarantine_stance.map(str)\n",
    "    test['label2'] = test.quarantine_argument.map(str)\n",
    "        \n",
    "    with open(os.path.join(pathname, 'test_quarantine_tokens.json'), 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            json.dumps(i) for i in test[['text', 'label1', 'label2']].to_dict('records')) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/convbert_quarantine_0.jsonnet\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/convbert_quarantine_0.jsonnet\n",
    "\n",
    "local embedding_dim = 768;\n",
    "local foldnum = 0;\n",
    "local max_length = 512;\n",
    "local lr = std.parseJson(std.extVar('lr'));\n",
    "local dropout = std.parseJson(std.extVar('dropout'));\n",
    "local batch_size = std.parseJson(std.extVar('batch_size'));\n",
    "local dataset_reader_type = \"models_scripts.dataset_readers.two_outputs_rdr.TwoOutputsTextClassificationJsonReader\";\n",
    "local model_type = \"models_scripts.models.two_outputs_clf.TwoOutputsTextClassifier\";\n",
    "local model_name = \"DeepPavlov/rubert-base-cased-conversational\";\n",
    "\n",
    "{\n",
    "  vocabulary: {\n",
    "    non_padded_namespaces: [\"tokens\", \"labels1\", \"labels2\"]\n",
    "  },\n",
    "  dataset_reader: {\n",
    "      type: dataset_reader_type,\n",
    "      num_labels1: 4,\n",
    "      num_labels2: 4,\n",
    "      tokenizer: {\n",
    "        type: \"pretrained_transformer\",\n",
    "        model_name: model_name,\n",
    "        max_length: max_length,\n",
    "      },\n",
    "      token_indexers: {\n",
    "          tokens: {\n",
    "            type: \"pretrained_transformer\",\n",
    "            model_name: model_name,\n",
    "            max_length: max_length,\n",
    "            namespace: 'tokens',\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  train_data_path: 'data/fold_' + foldnum + '/train_quarantine_tokens.json',\n",
    "  validation_data_path: 'data/fold_' + foldnum + '/test_quarantine_tokens.json',\n",
    "  model: {\n",
    "    type: model_type,\n",
    "    dropout: dropout,\n",
    "    num_labels1: 4,\n",
    "    num_labels2: 4,\n",
    "    label1_weights: [0.1, 0.1, 0.3, 1.0],\n",
    "    label2_weights: [0.1, 0.1, 0.6, 1.0],\n",
    "    text_field_embedder: {\n",
    "        token_embedders: {\n",
    "            tokens: {\n",
    "              type: \"pretrained_transformer\",\n",
    "              model_name: model_name,\n",
    "              max_length: max_length,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    seq2vec_encoder: {\n",
    "        type: \"bert_pooler\",\n",
    "           pretrained_model: model_name,\n",
    "           requires_grad: true,\n",
    "           dropout: 0.2,\n",
    "    }\n",
    "  },\n",
    "  data_loader: {\n",
    "    batch_sampler: {\n",
    "      type: 'bucket',\n",
    "      batch_size: batch_size,\n",
    "    },\n",
    "  },\n",
    "  trainer: {\n",
    "    optimizer: {\n",
    "      type: \"huggingface_adamw\",\n",
    "      lr: lr,\n",
    "      weight_decay: 0.1,\n",
    "    },\n",
    "    learning_rate_scheduler: {\n",
    "      type: \"slanted_triangular\",\n",
    "      cut_frac: 0.06\n",
    "    },\n",
    "    validation_metric: '+all_mean',\n",
    "    num_serialized_models_to_keep: 1,\n",
    "    num_epochs: 30,\n",
    "    patience: 3,\n",
    "    cuda_device: 1,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tune_convbert_q.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile tune_convbert_q.sh\n",
    "\n",
    "export METHOD=convbert_quarantine\n",
    "rm -r $METHOD\n",
    "mkdir $METHOD\n",
    "export FOLD=0\n",
    "export STUDY_NAME=convbert_test10\n",
    "# optuna delete-study --study-name $STUDY_NAME\n",
    "allennlp tune configs/${METHOD}_${FOLD}.jsonnet configs/base_model_params.json --serialization-dir $METHOD/fold_${FOLD} \\\n",
    "    --study-name $STUDY_NAME \\\n",
    "    --skip-if-exists \\\n",
    "    --metrics best_validation_all_mean \\\n",
    "    --direction maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-04 21:00:09,211 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2022-02-04 21:00:09,834 - INFO - allennlp.common.plugins - Plugin allennlp_optuna available\n",
      "batch_size=6 dropout=0.2992458712067729 lr=1.1411634348494446e-05\n"
     ]
    }
   ],
   "source": [
    "! allennlp best-params --study-name convbert_test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_top_trials('convbert_quarantine/fold_0/trial_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting convbert_quarantine.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile convbert_quarantine.sh\n",
    "\n",
    "export METHOD=convbert_quarantine\n",
    "rm -r $METHOD\n",
    "mkdir $METHOD\n",
    "export batch_size=6\n",
    "export dropout=0.3\n",
    "export lr=1e-05\n",
    "\n",
    "\n",
    "python utils/make_k_copies.py --filename configs/${METHOD}_0.jsonnet --k 5\n",
    "\n",
    "export FOLD=0\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=1\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=2\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=3\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "\n",
    "export FOLD=4\n",
    "allennlp train -s $METHOD/fold_${FOLD} configs/${METHOD}_${FOLD}.jsonnet\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_test.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/test_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts\n",
    "                 \n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "                 --output-file $METHOD/fold_${FOLD}/predictions_train.json $METHOD/fold_${FOLD}/model.tar.gz \\\n",
    "                 data/fold_${FOLD}/train_quarantine_tokens.json \\\n",
    "                 --include-package models_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks stance: 60.03, argument: 67.96\n",
      "Vaccines stance: 58.88, argument: 39.68\n",
      "Quarantine stance: 42.39, argument: 42.72\n",
      "Fold 0 ------------------\n",
      "F1 stance\t::: 59.45\n",
      "F1 argument\t::: 53.82\n",
      "Masks stance: 61.89, argument: 69.94\n",
      "Vaccines stance: 56.96, argument: 59.94\n",
      "Quarantine stance: 46.35, argument: 43.7\n",
      "Fold 1 ------------------\n",
      "F1 stance\t::: 59.42\n",
      "F1 argument\t::: 64.94\n",
      "Masks stance: 59.47, argument: 56.58\n",
      "Vaccines stance: 52.76, argument: 48.53\n",
      "Quarantine stance: 54.79, argument: 51.01\n",
      "Fold 2 ------------------\n",
      "F1 stance\t::: 56.12\n",
      "F1 argument\t::: 52.56\n",
      "Masks stance: 61.68, argument: 69.33\n",
      "Vaccines stance: 47.73, argument: 37.15\n",
      "Quarantine stance: 38.12, argument: 29.76\n",
      "Fold 3 ------------------\n",
      "F1 stance\t::: 54.7\n",
      "F1 argument\t::: 53.24\n",
      "Masks stance: 62.32, argument: 64.1\n",
      "Vaccines stance: 45.8, argument: 46.69\n",
      "Quarantine stance: 58.93, argument: 52.75\n",
      "Fold 4 ------------------\n",
      "F1 stance\t::: 54.06\n",
      "F1 argument\t::: 55.4\n"
     ]
    }
   ],
   "source": [
    "f1_stances = []\n",
    "f1_arguments = []\n",
    "\n",
    "f1_masks_stances = []\n",
    "f1_masks_arguments = []\n",
    "f1_vaccines_stances = []\n",
    "f1_vaccines_arguments = []\n",
    "f1_quarantine_stances = []\n",
    "f1_quarantine_arguments = []\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    pathname = f'data/fold_{fold}'\n",
    "    \n",
    "    with open(f'data/fold_{fold}/test_tokens.json', 'r') as file:\n",
    "        test = pd.read_json(file.read(), lines=True)\n",
    "        \n",
    "    with open(f'convbert_masks/fold_{fold}/predictions_test.json', 'r') as file:\n",
    "        pred = pd.read_json(file.read(), lines=True)\n",
    "        pred_masks_stance = pred.label1\n",
    "        pred_masks_argument = pred.label2\n",
    "\n",
    "    f1_masks_stance = fine_grained_f1(test.masks_stance, pred_masks_stance)\n",
    "    f1_masks_argument = fine_grained_f1(test.masks_argument, pred_masks_argument)\n",
    "    print(f'Masks stance: {(f1_masks_stance*100).round(2)}, argument: {(f1_masks_argument*100).round(2)}')\n",
    "    f1_masks_stances.append(f1_masks_stance)\n",
    "    f1_masks_arguments.append(f1_masks_argument)\n",
    "    \n",
    "    with open(f'convbert_vaccines/fold_{fold}/predictions_test.json', 'r') as file:\n",
    "        pred = pd.read_json(file.read(), lines=True)\n",
    "        pred_vac_stance = pred.label1\n",
    "        pred_vac_argument = pred.label2\n",
    "        \n",
    "    f1_vac_stance = fine_grained_f1(test.vaccines_stance, pred_vac_stance)\n",
    "    f1_vac_argument = fine_grained_f1(test.vaccines_argument, pred_vac_argument)\n",
    "    print(f'Vaccines stance: {(f1_vac_stance*100).round(2)}, argument: {(f1_vac_argument*100).round(2)}')\n",
    "    f1_vaccines_stances.append(f1_vac_stance)\n",
    "    f1_vaccines_arguments.append(f1_vac_argument)\n",
    "    \n",
    "    with open(f'convbert_quarantine/fold_{fold}/predictions_test.json', 'r') as file:\n",
    "        pred = pd.read_json(file.read(), lines=True)\n",
    "        pred_quarantine_stance = pred.label1\n",
    "        pred_quarantine_argument = pred.label2\n",
    "        \n",
    "    f1_quarantine_stance = fine_grained_f1(test.quarantine_stance, pred_quarantine_stance)\n",
    "    f1_quarantine_argument = fine_grained_f1(test.quarantine_argument, pred_quarantine_argument)\n",
    "    print(f'Quarantine stance: {(f1_quarantine_stance*100).round(2)}, argument: {(f1_quarantine_argument*100).round(2)}')\n",
    "    f1_quarantine_stances.append(f1_quarantine_stance)\n",
    "    f1_quarantine_arguments.append(f1_quarantine_argument)\n",
    "\n",
    "    f1_stance = average_f1([f1_masks_stance, f1_vac_stance])\n",
    "    f1_arg = average_f1([f1_masks_argument, f1_vac_argument])\n",
    "    \n",
    "    print(f'Fold {fold} ------------------')\n",
    "    print('F1 stance\\t:::', (f1_stance * 100).round(2))\n",
    "    print('F1 argument\\t:::', (f1_arg * 100).round(2))\n",
    "    \n",
    "    f1_stances.append(f1_stance)\n",
    "    f1_arguments.append(f1_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stance \t\t56.8 ± 2.3\n",
      "argument\t55.99121114449 ± 4.6\n"
     ]
    }
   ],
   "source": [
    "print(f'stance \\t\\t{(np.mean(f1_stances) * 100).round(1)} ± {(np.std(f1_stances) * 100).round(1)}')\n",
    "print(f'argument\\t{(np.mean(f1_arguments) * 100).round(12)} ± {(np.std(f1_arguments) * 100).round(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask stance \t61.1 ± 1.1\n",
      "mask argument\t65.6 ± 4.9\n"
     ]
    }
   ],
   "source": [
    "print(f'mask stance \\t{(np.mean(f1_masks_stances) * 100).round(1)} ± {(np.std(f1_masks_stances) * 100).round(1)}')\n",
    "print(f'mask argument\\t{(np.mean(f1_masks_arguments) * 100).round(1)} ± {(np.std(f1_masks_arguments) * 100).round(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaccine stance \t\t52.4 ± 5.1\n",
      "vaccine argument\t46.4 ± 8.0\n"
     ]
    }
   ],
   "source": [
    "print(f'vaccine stance \\t\\t{(np.mean(f1_vaccines_stances) * 100).round(1)} ± {(np.std(f1_vaccines_stances) * 100).round(1)}')\n",
    "print(f'vaccine argument\\t{(np.mean(f1_vaccines_arguments) * 100).round(1)} ± {(np.std(f1_vaccines_arguments) * 100).round(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quarantine stance\t48.1 ± 7.7\n",
      "quarantine argument\t44.0 ± 8.1\n"
     ]
    }
   ],
   "source": [
    "print(f'quarantine stance\\t{(np.mean(f1_quarantine_stances) * 100).round(1)} ± {(np.std(f1_quarantine_stances) * 100).round(1)}')\n",
    "print(f'quarantine argument\\t{(np.mean(f1_quarantine_arguments) * 100).round(1)} ± {(np.std(f1_quarantine_arguments) * 100).round(1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict on unlabeled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('data/val_empty.tsv', sep='\\t')\n",
    "dev['tokens'] = dev.text.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = [model.predict(text, k=DEPTH) for idx, text in dev.tokens.iteritems()]\n",
    "pred_masks_stance = [load_prediction('masks_stance', pred) for pred in model_predictions]\n",
    "pred_masks_argument = [load_prediction('masks_argument', pred) for pred in model_predictions]\n",
    "pred_quarantine_stance = [load_prediction('quarantine_stance', pred) for pred in model_predictions]\n",
    "pred_quarantine_argument = [load_prediction('quarantine_argument', pred) for pred in model_predictions]\n",
    "pred_vac_stance = [load_prediction('vaccines_stance', pred) for pred in model_predictions]\n",
    "pred_vac_argument = [load_prediction('vaccines_argument', pred) for pred in model_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.masks_stance = pred_masks_stance\n",
    "dev.masks_argument = pred_masks_argument\n",
    "dev.quarantine_stance = pred_quarantine_stance\n",
    "dev.quarantine_argument = pred_quarantine_argument\n",
    "dev.vaccines_stance = pred_vac_stance\n",
    "dev.vaccines_argument = pred_vac_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev[['text_id', 'text', 'masks_stance', 'masks_argument', 'quarantine_stance', 'quarantine_argument',\n",
    "     'vaccines_stance', 'vaccines_argument']].to_csv('data/baseline_dev.tsv', sep='\\t', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
